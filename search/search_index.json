{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"ai_for_science/automation_bottleneck/","title":"Automation Bottleneck","text":"<p>Many AI-for-science workflows bottleneck on automation, not model capability: data movement, provenance, experiment orchestration, and reliable evaluation.</p>"},{"location":"ai_for_science/automation_bottleneck/#a-simple-checklist","title":"A simple checklist","text":"<ul> <li>Inputs are versioned (data + prompts + code)</li> <li>Runs are reproducible (configs + seeds + environments)</li> <li>Results are queryable (metadata + artifacts)</li> <li>Failures are observable (logs + traces)</li> </ul> <pre><code>sequenceDiagram\n  participant R as Researcher\n  participant O as Orchestrator\n  participant C as Compute\n  R-&gt;&gt;O: define experiment\n  O-&gt;&gt;C: run sweep\n  C--&gt;&gt;O: metrics + artifacts\n  O--&gt;&gt;R: report + provenance</code></pre>"}]}